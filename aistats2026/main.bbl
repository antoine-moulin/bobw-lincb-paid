\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and Szepesv{\'a}ri]{Abbasi-YadkoriPS11}
Y.~Abbasi-Yadkori, D.~P{\'a}l, and C.~Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages 2312--2320, 2011.

\bibitem[Abe and Long(1999)]{AbeL99}
N.~Abe and P.~M. Long.
\newblock Associative reinforcement learning using linear probabilistic concepts.
\newblock In \emph{Proceedings of the Sixteenth International Conference on Machine Learning}, pages 3--11. Morgan Kaufmann, 1999.

\bibitem[Abeille and Lazaric(2017)]{AbeilleL17}
M.~Abeille and A.~Lazaric.
\newblock Linear thompson sampling revisited.
\newblock In \emph{Proceedings of the 20th International Conference on Artificial Intelligence and Statistics}, volume~54, pages 176--184. PMLR, 2017.

\bibitem[Abeille et~al.(2025)Abeille, Janz, and Pike-Burke]{AbeilleJP25}
M.~Abeille, D.~Janz, and C.~Pike-Burke.
\newblock When and why randomised exploration works (in linear bandits).
\newblock In \emph{Proceedings of the International Conference on Algorithmic Learning Theory}, volume 272, pages 4--22. PMLR, 2025.

\bibitem[Agrawal and Goyal(2013)]{AgrawalG13TSlinear}
S.~Agrawal and N.~Goyal.
\newblock Thompson sampling for contextual bandits with linear payoffs.
\newblock In \emph{Proceedings of the 30th International Conference on Machine Learning}, pages 127--135. JMLR.org, 2013.

\bibitem[Alon et~al.(2013)Alon, Cesa-Bianchi, Gentile, and Mansour]{AlonCGM13}
N.~Alon, N.~Cesa-Bianchi, C.~Gentile, and Y.~Mansour.
\newblock From bandits to experts: A tale of domination and independence.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages 1610--1618, 2013.

\bibitem[Amir et~al.(2022)Amir, Azov, Koren, and Livni]{amir2022better}
I.~Amir, G.~Azov, T.~Koren, and R.~Livni.
\newblock Better best of both worlds bounds for bandits with switching costs.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 15800--15810, 2022.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and Schapire]{auer2002nonstochastic}
P.~Auer, N.~Cesa-Bianchi, Y.~Freund, and R.~E. Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM Journal on Computing}, 32\penalty0 (1):\penalty0 48--77, 2002.

\bibitem[Avner et~al.(2012)Avner, Mannor, and Shamir]{AvnerMS12}
O.~Avner, S.~Mannor, and O.~Shamir.
\newblock Decoupling exploration and exploitation in multi-armed bandits.
\newblock In \emph{Proceedings of the 29th International Conference on Machine Learning}, 2012.

\bibitem[Bart{\'o}k et~al.(2014)Bart{\'o}k, Foster, P{\'a}l, Rakhlin, and Szepesv{\'a}ri]{bartok2014partial}
G.~Bart{\'o}k, D.~Foster, D.~P{\'a}l, A.~Rakhlin, and C.~Szepesv{\'a}ri.
\newblock Partial monitoring---classification, regret bounds, and algorithms.
\newblock \emph{Mathematics of Operations Research}, 39\penalty0 (4):\penalty0 967--997, 2014.
\newblock \doi{10.1287/moor.2014.0663}.

\bibitem[Bastani et~al.(2021)Bastani, Bayati, and Khosravi]{BastaniBK21}
H.~Bastani, M.~Bayati, and K.~Khosravi.
\newblock Mostly exploration-free algorithms for contextual bandits.
\newblock \emph{Management Science}, 67\penalty0 (3):\penalty0 1329--1349, 2021.
\newblock \doi{10.1287/mnsc.2020.3605}.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and Schapire]{BeygelzimerLLRS11}
A.~Beygelzimer, J.~Langford, L.~Li, L.~Reyzin, and R.~E. Schapire.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \emph{Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}, volume~15, pages 19--26. JMLR.org, 2011.

\bibitem[Bubeck and Cesa-Bianchi(2012)]{BubeckCbook12}
S.~Bubeck and N.~Cesa-Bianchi.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit problems.
\newblock \emph{Foundations and Trends in Machine Learning}, 5\penalty0 (1):\penalty0 1--122, 2012.
\newblock \doi{10.1561/2200000024}.

\bibitem[Bubeck and Slivkins(2012)]{bubeck12bobw}
S.~Bubeck and A.~Slivkins.
\newblock The best of both worlds: Stochastic and adversarial bandits.
\newblock In \emph{Proceedings of the 25th Annual Conference on Learning Theory}, volume~23, pages 42.1--42.23. PMLR, 2012.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{DaniHK08}
V.~Dani, T.~P. Hayes, and S.~M. Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In \emph{Proceedings of the 21st Annual Conference on Learning Theory}, pages 355--366. Omnipress, 2008.

\bibitem[Dann et~al.(2023)Dann, Wei, and Zimmert]{DannWZ23}
C.~Dann, C.-Y. Wei, and J.~Zimmert.
\newblock A blackbox approach to best of both worlds in bandits and beyond.
\newblock In \emph{Proceedings of the 36th Annual Conference on Learning Theory}, volume 195, pages 5503--5570. PMLR, 2023.

\bibitem[Degenne et~al.(2020)Degenne, Shao, and Koolen]{DegenneSK20}
R.~Degenne, H.~Shao, and W.~M. Koolen.
\newblock Structure adaptive algorithms for stochastic bandits.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119, pages 2443--2452. PMLR, 2020.

\bibitem[Efroni et~al.(2021)Efroni, Merlis, Saha, and Mannor]{EfroniMSM21}
Y.~Efroni, N.~Merlis, A.~Saha, and S.~Mannor.
\newblock Confidence-budget matching for sequential budgeted learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139, pages 2937--2947. PMLR, 2021.

\bibitem[Flynn et~al.(2023)Flynn, Reeb, Kandemir, and Peters]{FlynnRKP23}
H.~Flynn, D.~Reeb, M.~Kandemir, and J.~R. Peters.
\newblock Improved algorithms for stochastic linear bandits using tail bounds for martingale mixtures.
\newblock In \emph{Advances in Neural Information Processing Systems 36}, 2023.

\bibitem[Jin et~al.(2023)Jin, Liu, and Luo]{jin2023improved}
T.~Jin, J.~Liu, and H.~Luo.
\newblock Improved best-of-both-worlds guarantees for multi-armed bandits: Ftrl with general regularizers and multiple optimal arms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 30918--30978, 2023.

\bibitem[Kato and Ito(2025)]{KatoI25}
M.~Kato and S.~Ito.
\newblock Lc-tsallis-inf: Generalized best-of-both-worlds linear contextual bandits.
\newblock In \emph{Proceedings of the International Conference on Artificial Intelligence and Statistics}, volume 258, pages 3655--3663. PMLR, 2025.

\bibitem[Kirschner et~al.(2020)Kirschner, Lattimore, and Krause]{KirschnerL020}
J.~Kirschner, T.~Lattimore, and A.~Krause.
\newblock Information directed sampling for linear partial monitoring.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, volume 125, pages 2328--2369. PMLR, 2020.

\bibitem[Kuroki et~al.(2024)Kuroki, Rumi, Tsuchiya, Vitale, and Cesa-Bianchi]{BOBWlinear}
Y.~Kuroki, A.~Rumi, T.~Tsuchiya, F.~Vitale, and N.~Cesa-Bianchi.
\newblock Best-of-both-worlds algorithms for linear contextual bandits.
\newblock In S.~Dasgupta, S.~Mandt, and Y.~Li, editors, \emph{Proceedings of the 27th International Conference on Artificial Intelligence and Statistics}, volume 238 of \emph{Proceedings of Machine Learning Research}, pages 1216--1224. PMLR, 2024.

\bibitem[Langford and Zhang(2007)]{LangfordZ07}
J.~Langford and T.~Zhang.
\newblock The epoch-greedy algorithm for multi-armed bandits with side information.
\newblock In \emph{Advances in Neural Information Processing Systems 20}, pages 817--824, 2007.

\bibitem[Lattimore and Szepesv{\'a}ri(2017)]{LattimoreS17}
T.~Lattimore and C.~Szepesv{\'a}ri.
\newblock The end of optimism? an asymptotic analysis of finite-armed linear bandits.
\newblock In \emph{Proceedings of the 20th International Conference on Artificial Intelligence and Statistics}, volume~54, pages 728--737. PMLR, 2017.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{BanditBook}
T.~Lattimore and C.~Szepesv{\'a}ri.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Li et~al.(2010)Li, Chu, Langford, and Schapire]{LiCLS10}
L.~Li, W.~Chu, J.~Langford, and R.~E. Schapire.
\newblock A contextual-bandit approach to personalized news article recommendation.
\newblock In \emph{Proceedings of the 19th International Conference on World Wide Web}, pages 661--670. ACM, 2010.
\newblock \doi{10.1145/1772690.1772758}.

\bibitem[Mannor and Shamir(2011)]{MannorS11}
S.~Mannor and O.~Shamir.
\newblock From bandits to experts: On the value of side-observations.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages 684--692, 2011.

\bibitem[Neu(2015)]{neu2016exploration}
G.~Neu.
\newblock Explore no more: Improved high-probability regret bounds for non-stochastic bandits.
\newblock \emph{arXiv preprint arXiv:1506.03271}, 2015.

\bibitem[Neu and Bart{\'o}k(2013)]{neu2013efficient}
G.~Neu and G.~Bart{\'o}k.
\newblock An efficient algorithm for learning with semi-bandit feedback.
\newblock \emph{arXiv preprint arXiv:1305.2732}, 2013.

\bibitem[Neu and Olkhovskaya(2020)]{NeuO20}
G.~Neu and J.~Olkhovskaya.
\newblock Efficient and robust algorithms for adversarial linear contextual bandits.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, volume 125, pages 3049--3068. PMLR, 2020.

\bibitem[Olkhovskaya et~al.(2023)Olkhovskaya, Mayo, van Erven, Neu, and Wei]{OlkhovskayaMENW23}
J.~Olkhovskaya, J.~J. Mayo, T.~van Erven, G.~Neu, and C.-Y. Wei.
\newblock First- and second-order bounds for adversarial linear contextual bandits.
\newblock In \emph{Advances in Neural Information Processing Systems 36}, 2023.

\bibitem[Rouyer et~al.(2022)Rouyer, van~der Hoeven, Cesa-Bianchi, and Seldin]{RouyerHCS22}
C.~Rouyer, D.~van~der Hoeven, N.~Cesa-Bianchi, and Y.~Seldin.
\newblock A near-optimal best-of-both-worlds algorithm for online learning with feedback graphs.
\newblock In \emph{Advances in Neural Information Processing Systems 35}, 2022.

\bibitem[Saha and Gaillard(2022)]{saha2022versatile}
A.~Saha and P.~Gaillard.
\newblock Versatile dueling bandits: Best-of-both world analyses for learning from relative preferences.
\newblock In \emph{Proceedings of the International Conference on Machine Learning}, pages 19011--19026. PMLR, 2022.

\bibitem[Seldin and Slivkins(2014)]{SeldinS14}
Y.~Seldin and A.~Slivkins.
\newblock One practical algorithm for both stochastic and adversarial bandits.
\newblock In \emph{Proceedings of the 31st International Conference on Machine Learning}, pages 1287--1295. JMLR.org, 2014.

\bibitem[Seldin et~al.(2014)Seldin, Bartlett, Crammer, and Abbasi-Yadkori]{paidobservations}
Y.~Seldin, P.~Bartlett, K.~Crammer, and Y.~Abbasi-Yadkori.
\newblock Prediction with limited advice and multiarmed bandits with paid observations.
\newblock In E.~Xing and T.~Jebara, editors, \emph{Proceedings of the 31st International Conference on Machine Learning}, volume~32 of \emph{Proceedings of Machine Learning Research}, pages 280--287, Beijing, China, 2014. PMLR.

\bibitem[Shalev-Shwartz(2012)]{ShalevShwartz12}
Shai Shalev-Shwartz.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0 (2):\penalty0 107--194, 2012.

\bibitem[Tsuchiya and Ito(2024)]{BOBWhardproblems}
T.~Tsuchiya and S.~Ito.
\newblock A simple and adaptive learning rate for ftrl in online learning with minimax regret of $\\theta(t^{2/3})$ and its application to best-of-both-worlds.
\newblock \emph{NeurIPS}, 2024.

\bibitem[Tsuchiya et~al.(2023)Tsuchiya, Ito, and Honda]{tsuchiya2023further}
T.~Tsuchiya, S.~Ito, and J.~Honda.
\newblock Further adaptive best-of-both-worlds algorithm for combinatorial semi-bandits.
\newblock In \emph{Proceedings of the International Conference on Artificial Intelligence and Statistics}, pages 8117--8144. PMLR, 2023.

\bibitem[Yun et~al.(2018)Yun, Prouti{\'e}re, Ahn, Shin, and Yi]{YunPASY18}
D.~Yun, A.~Prouti{\'e}re, S.~Ahn, J.~Shin, and Y.~Yi.
\newblock Multi-armed bandit with additional observations.
\newblock \emph{Proceedings of ACM Measurement and Analysis of Computing Systems}, 2\penalty0 (1):\penalty0 13:1--13:22, 2018.
\newblock \doi{10.1145/3179416}.

\bibitem[Zimmert and Marinov(2024)]{zimmert24prod}
J.~Zimmert and T.~V. Marinov.
\newblock Productive bandits: Importance weighting no more.
\newblock In \emph{Advances in Neural Information Processing Systems 37}, pages 85360--85388, 2024.

\bibitem[Zimmert and Seldin(2022)]{zimmerttsallis2022}
J.~Zimmert and Y.~Seldin.
\newblock Tsallis-inf: An optimal algorithm for stochastic and adversarial bandits, 2022.

\end{thebibliography}
