\section{REGRET ANALYSIS}\label{sec::regret}

We now introduce the main theoretical result of this work, which is that Algorithm~\ref{alg::FTRL_bobw} achieves Best-of-Both-Worlds regret guarantees in the setting of linear bandits with paid observations, under the assumptions introduced in Section~\ref{sec::setting}. 

\db{We have to change the dimension dependency after the fix.}
\begin{restatable}{theorem}{MainTheorem}\label{thm::main}
    In the adversarial regime, the regret of Algorithm~\ref{alg::FTRL_bobw} satisfies 
\begin{align*}
R_T 
\lesssim &
\left( \frac{cKd^2 \log K}{\lambda_{\min}^2} \right)^{1/3} T^{2/3} \\
&+ \sqrt{ \frac{\max(c,1)d \log K \cdot T}{\lambda_{\min}} } +\kappa
\end{align*}
with
\begin{align*}
\kappa &= \sqrt{\frac{cKd^2 \log K}{\lambda_{\min}^2}}+\frac{\max(c,1)d \log K}{\lambda_{\min}}\\
&+\frac{\max(c,1)K\log K}{\lambda_{\min}^2}+\frac{32Kd\sqrt{c}}{(1-\alpha)^2\sqrt{\beta_1}\min(1,\lambda_{\min})}\;.
\end{align*}
while in the corrupted stochastic regime with corruption level $C$ it satisfies
\begin{align*}
R_T 
&\lesssim
\frac{d\sqrt{\max(c,1)K \log K}}{\lambda_{\min}\deltamin^2} \cdot \log(T \deltamin^3) \\
&\quad + \left( \frac{C^2d \sqrt{\max(c,1)K \log K}}{\lambda_{\min}\deltamin^2} \cdot \log\left( \frac{T \deltamin}{C} \right) \right)^{1/3}\\
&\quad + \kappa + \kappa', \; \text{where we further define} \\
\kappa' &= \left(\left( \frac{cKd^2 \log K}{\lambda_{\min}^2} \right)^{1/3} +  \sqrt{ \frac{\max(c,1)d \log K}{\lambda_{\min}}}\right)\\
&\quad \times \left(\frac{1}{\deltamin^3} + \tfrac{C}{\deltamin}\right)^{2/3}
\end{align*}
\end{restatable}

This result shows that Algorithm~\ref{alg::FTRL_bobw} achieves the minimax-optimal $\mathcal{O}(T^{2/3})$ regret in the adversarial regime, while smoothly adapting to the (possibly corrupted) stochastic regime with logarithmic dependence on $T$ when $C = 0$. These bounds match the known lower bounds from \citep{paidobservations}, which applies to our setting since it encompasses the standard multi-armed bandit (by taking $d=1$ and $X_t=1$ a.s.), and extend the Best-of-Both-Worlds (BoBW) framework of \citep{BOBWhardproblems} to the setting of linear bandits.

While the dependence in $T$ is thus known to be optimal, the optimal dependence in other problem-specific parameters remains unknown, as this is the first work to address this setting. However, since our algorithm builds upon and generalizes both Algorithm~2 from \citep{BOBWlinear} and Algorithm~2 from \citep{BOBWhardproblems}, we can compare our regret bounds to theirs, even if the settings do not perfectly align. %in the special cases where the settings align, thereby providing evidence that our approach preserves the guarantees of existing methods in simpler regimes.

We consider first the limiting case where $c \to 0$, corresponding to the full-information setting, in which all losses are observed. In this regime, the first term of the adversarial regret bound vanishes, and we have %the dominant term becomes the one scaling as $\sqrt{dT}$, and so
\[
R_T \lesssim \sqrt{ \frac{\log (K)\cdot d T}{\lambda_{\min}} }.
\]
This matches, up to logarithmic factors, the adversarial regret bound established for Algorithm~2 in \citep{BOBWlinear}, namely 
\[
R_T\lesssim \sqrt{T\left(d + \frac{\log T}{\lambda_{\min}}\right) K \log K \log T}.
\]
In our case, the factor \(K\) is replaced by \(\log K\), which reflects the full-information nature of our setting, a standard improvement in such regimes.
However, in the stochastic regime, our regret exhibits an additional 
$\frac{1}{\deltamin}$ factor compared to the full-information bounds 
in \citep{BOBWlinear}. 
But on the countrary, our algorithm has a better $\log T$ dependence, 
thus our bound is better if $T$ is significantly larger than $\frac{1}{\deltamin}$. 
Although, we do not know whether our improved $\log T$ dependency stems from being in the full-information setting or from other factors. We can at least observe that the dependence on the setting-specific parameters \( d \) and \( \lambda_{\min} \) in our bounds matches that of their Algorithm~2.

Another useful comparison is to consider the special case $d=1, \cX=\{1\}$,  %swhere the context space has size one, which implies \( \lambda_{\min} = 1 \). 
in which case we recover the setting of \cite{SeldinS14}.
%, similar to Algorithm~2 from \citep{BOBWhardproblems}.
From their Corollary~17,  Algorithm~2 of \cite{BOBWhardproblems} obtain an adversarial regret bound of
\[
\cR_T\lesssim \left((cK)^{1/3} T^{2/3} (\log K)^{1/3} \right),
\]
which is exactly the scaling that we obtain with Theorem~\ref{thm::main} in this setting. This observation furthermore still holds in the stochastic setting. 
% \[
% O\left( \left( \frac{cK \log K}{\lambda_{\min}^2} \right)^{1/3} T^{2/3} \right),
% \]
%Hence, in that setting the bounds match.
% In the stochastic regime, correcting a small typo in their expression (where \( \rho \) should be replaced by \( \rho^2 \)), their regret bound becomes
% \[
% O\left( \frac{ \sqrt{cK \log K} }{ \deltamin^2 } \log(T \deltamin^3) \right),
% \]
% while ours reads
% \[
% O\left( \frac{ \sqrt{cK \log K} }{ \lambda_{\min} \deltamin^2 } \log(T \deltamin^3) \right),
% \]
% which again coincides when \( \lambda_{\min} = 1,d=1 \).

These comparisons suggest that, while we can not establish optimality in general due to the lack of known lower bounds, our algorithm can be viewed as a strict generalization of the approach in \citep{BOBWhardproblems} for bandits with paid observations, since we recover their guarantees in this setting. Moreover, since the dependencies in \( d \) and \( \lambda_{\min} \) are known to be optimal compared to previous approaches when \( c = 0 \), this further supports the relevance of our design beyond prior approaches.



%\red{[Before moving to the proof part, there should be a discussion on the result (Theorem 1) itself. Here we could mention, for instance, that compared to the works we build upon,  our bound recovers the full-info setting when $c=0$. Here you can give a high-level discussion of how you managed to get rid of the factor $1/c$, pointing to the specific part in the proof sketch (section below) where this is explained.] \\

%\red{[About above comments: please expand at least a bit the discussion. I agree that the case $c=0$ should be discussed, since it's an interesting critical regime. Is the stochastic regret $0$ when $c=0$? I suspect there are missing additional constants in the bounds. If the bound is independent of $T$ in the stochastic regime when $c=0$, this is something super interesting to discuss. This kind of discussion is also usually time to thoroughly compare with existing related results (citing papers). You started a bit but it's not enough, here we have no explanation about the quality of the stochastic bound for instance.]}

A detailed proof of the theorem can be found in Appendix~\ref{AppendixRegret}. In the following, we present the main steps of the proofs, highlighting the technical arguments that required to be adapted from the existing frameworks.

\begin{proof}[Proof sketch] As a preliminary step of the analysis, we isolate the difficulty induced by the use of (biased) MGR estimates (Eq.~\eqref{eq::MGR_estimate}) instead of using the unbiased estimators from Eq.~\eqref{eq::estimator}. Following the proof technique of \cite{BOBWlinear}, we introduce an auxiliary game where these estimators are treated as unbiased, and for which the regret would thus become
\[
\tilde{R}_T := \mathbb{E}\!\left[ \sum_{t=1}^T 
\lan{X_t, \tilde{\theta}_{t, A_t}} 
- \lan{X_t, \tilde{\theta}_{t, \pi^*(X_t)}} \right].
\]
We can verify that the actual regret of our algorithm thus satisfies
\[
R_T \leq \tilde{R}_T %\mathbb{E}[\tilde{R}_T] 
+ 2 \sum_{t=1}^T \max_{a \in [K]} \Big| 
\mathbb{E}\!\left[ \lan{X_t, \tilde{\theta}_{t,a} - \theta_{t,a}} \right] 
\Big|. 
\]
Then, in Lemma~\ref{lem::boundBias} we prove that the second term of this upper bound can be upper bounded by a constant, independent of all problem parameters. In the following, we thus focus on upper bounding $\tilde{R}_T$. We write the following proof steps with the notation $R_T$, with an abuse of notation, since previous result showed that both terms have the same scaling in $T$.  

The remainder of the analysis builds on the general framework introduced by Tsuchiya and Ito~\citep{BOBWhardproblems} to build Best-of-Both-Worlds algorithms for problems with minimax regret scaling with $T^{2/3}$, and in particular their instantiation of this framework to tackle standard multi-armed bandit with paid observations (without the linear contextual structure).
Our first contribution is an adaptation of their Theorem 7 to accommodate the linear contextual structure, that we introduce below. 
\begin{restatable}[Adaptation of Theorem~7 of \citep{BOBWhardproblems}]{lemma}{adath}\label{lem::adapt_tsuchaya}
Suppose that Algorithm~\ref{alg::FTRL_bobw} satisfies the following conditions in the adversarial regime:
\begin{enumerate}[label=(\roman*),ref=(\roman*)]
    \item $R_T \leq \sum_{t=1}^T \bE\Bigg[\Big(\tfrac{1}{\eta_{t}}-\tfrac{1}{\eta_{t-1}}\Big)h_{t} 
        +  \tfrac{z_t \eta_t}{\gamma_t} 
        +  \gamma_t \Bigg] + \bar{\beta}\bar{h} \,,$
    \item $\bE[h_{t+1}\mid\cH_{t}] \leq 2\,\bE[h_t\mid\cH_{t-1}]$ for all $t \geq 1$.
\end{enumerate}

Then the regret can be bounded as
\[
R_T \lesssim (z_{\max} h_1)^{1/3} T^{2/3} + \sqrt{u_{\max} h_1 T} + \kappa,
\]
where
\begin{align*}
z_{\max} &= \max_{t\in[T]} z_t \leq 4cK\log K\frac{1}{\lambda_{\min}^2},\\
u_{\max} &= \max_{t\in[T]} u_t \leq 4\max(c,1)\log K\frac{1}{\lambda_{\min}},
\end{align*}
and
\[
\kappa := \sqrt{z_{\max}\eta_1} + u_{\max}\eta_1 + \tfrac{h_1}{\eta_1} + \bar{\beta} h_{\max}.
\]

Moreover, if Algorithm~\ref{alg::FTRL_bobw} satisfies the following conditions in the stochastic regime: there exists a constant $\rho>0$ such that, $\forall t\geq 1$,
\begin{enumerate}[label=(\roman*),ref=(\roman*),start=3]
    \item $\sqrt{z_t h_t} \leq \sqrt{\rho}\,\big(1 - \pi_T^{\star}(X_t)\mid X_t\big)$, and
    \item $u_t h_t \leq \rho\,\big(1 - \pi_T^{\star}(X_t)\mid X_t\big)$, 
\end{enumerate}
then, for $T \geq \tau := \tfrac{1}{\deltamin^3} + \tfrac{C}{\deltamin}$ it holds that
\begin{align*}
R_T & \lesssim
    \frac{\rho}{\deltamin^2}\log(T \deltamin^3)+ \Big(\tfrac{C^2 \rho}{\deltamin^2} \log(\tfrac{T \deltamin}{C})\Big)^{1/3}
    + \kappa'
\end{align*}
with
\[
\kappa' := \kappa + \big((z_{\max} h_1)^{1/3} + \sqrt{u_{\max} h_1}\big)\,
\Big(\tfrac{1}{\deltamin^3} + \tfrac{C}{\deltamin}\Big)^{2/3}.
\]
\end{restatable}

While Lemma~\ref{lem::adapt_tsuchaya} adapts Theorem 7 from \citep{BOBWhardproblems}, it differs in several significant aspects. First, condition~(i) is new and replaces conditions~(i)--(ii) in the original theorem, and both lead to a similar proof structure, our condition better adjust the framework to our setting.  
Second, condition~(ii) is a relaxed reformulation of condition~(iii) in \citep{BOBWhardproblems}, which is necessary to handle the stochasticity of contexts in our setting. With careful use of the tower rule, we show that this weaker assumption is sufficient for the regret analysis. Finally, conditions~(iii) and (iv) are reformulations of conditions~(iv) and (v) from \citep{BOBWhardproblems}, and the corresponding proof techniques carry over with only little modifications. The detailed proof of this lemma is deferred to Appendix~\ref{Appendix}.

To establish Theorem~\ref{thm::main}, it then suffices to verify that Algorithm~\ref{alg::FTRL_bobw} satisfies each of the four conditions. 

Condition~(i) follows from the standard FTRL regret decomposition: the stability term bound is direct to obtain, while the penalty term is controlled using Lemma~\ref{lem::ub_ftrl_technical} (in Appendix)% on $|l_t\eta_t|$
, which is similarly to the proof of \citep[Theorem 8]{BOBWhardproblems}. 

We prove condition~(ii) in Lemma~\ref{lem::cond_bound_ht}. The proof consists in applying Lemma 15 from \citep{BOBWhardproblems} (restated as Lemma~\ref{lem::ftrl_smooth_bound}) for each fixed context, and to conclude via linearity of expectation. A key challenge arises from the fact that, in our setting, we have the bound $\mathbb{E}\left[\langle X_t, \hat{\theta}_{t,a} \rangle^2\right] \leq \frac{1}{\lambda_{\min}^2 p_t}$, which contrasts with the original bound $\mathbb{E}\left[l_t^2\right] \leq \frac{1}{p_t}$ in the non-contextual case. Since Lemma~\ref{lem::ftrl_smooth_bound} only accommodates a constant upper bound, this discrepancy required a careful adjustment of several parameters, specifically $u_t$ and $\bar{\beta}$, which represents a slight modification in the precise behavior of the algorithm.

Finally, Conditions~(iii) and (iv) are verified by combining entropy bounds from \citep{BOBWhardproblems} with direct control of the variance-like quantities $z_t$ and $u_t$, thereby linking them to the optimal action probability.  

Together, these arguments ensure that Algorithm~\ref{alg::FTRL_bobw} satisfies the assumptions of Lemma~\ref{lem::adapt_tsuchaya}, which directly yields the regret guarantees stated in Theorem~\ref{thm::main}.

The full derivations and supporting lemmas are deferred to Appendix~\ref{AppendixRegret}, where we carefully establish that each condition of the lemma holds in our setting. 
\end{proof}

While the definition of \( p_t \) in \citep{BOBWhardproblems} differs from ours by a factor \( (cK)^{-1} \), this appears to be a simple typo in their presentation. Indeed, their analysis assumes \( p_t = \frac{1}{cK} (\sqrt{z_t \eta_t} + u_t \eta_t) \), even though the statement of their Algorithm~2 defines \( p_t := \sqrt{z_t \eta_t} + u_t \eta_t \). %However, this normalization is necessary to ensure consistency with their analysis, which assumes \( \gamma_t = cK p_t = \sqrt{z_t \eta_t} + u_t \eta_t \). 
We can use this observation to comment on the optimality of the tuning of $p_t$ with respect to the analysis used to derive BoBW regret bounds for our algorithm. %
%Although likely unintentional, this mismatch has meaningful consequences: it introduces a discrepancy in key steps of the analysis, potentially leading to regret bounds that are looser by a factor of \( cK + \frac{1}{cK} \). However, we can use this observation to comment on the optimality of the tuning of $p_t$ with respect to the analysis used to derive BoBW regret bounds for our algorithm.
%To see this, consider 
Indeed, a step in the analysis (see Eq.~\eqref{eq::critical step}) involves the quantity $\gamma_t' := \gamma_t - \frac{u_t}{\beta_t}$. With our definition, this yields $\gamma_t' = \sqrt{z_t / \beta_t}$, while using the unnormalized $p_t$ (without $1/(cK)$) gives
\[
\gamma_t' = cK \sqrt{z_t \eta_t} + (cK - 1) u_t \eta_t \geq cK \sqrt{z_t \eta_t},
\]
assuming $cK \geq 1$. This leads to the bound
\begin{align*}
\sum_{t=1}^T \bE\left[\frac{z_t\eta_t}{\gamma_t'}+\gamma_t\right]
& \leq \sum_{t=1}^T \bE\left[ \frac{1}{cK}\sqrt{\frac{z_t}{\beta_t}} + cK\left(\sqrt{\frac{z_t}{\beta_t}} + \frac{u_t}{\beta_t}\right)\right] \\
& \leq \left(\frac{1}{cK} + cK\right) \sum_{t=1}^T \bE\left[2 \sqrt{\frac{z_t}{\beta_t}} + \frac{u_t}{\beta_t}\right] \;.
\end{align*}
The factor $(cK)^{-1}+cK$ then propagates through the analysis and degrades the regret bound. More generally, an overestimation of \( p_t \) by a multiplicative factor \( \omega \) leads to a regret that is worsened by a factor proportional to \( \omega + \omega^{-1} \), so $\omega=1$ (our tuning) is optimal.


