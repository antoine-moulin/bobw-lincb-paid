\section{To-Do}

\db{This section is for me to list the mistakes I remarked in the proof and some thoughts from related work.}

\begin{itemize}
    \item \DB{We have to cleanly re-write the proof with the ghost sample technique}, which is probably going to simplify some things (e.g. Lemma 6, maybe the property is only needed for the ghost context). To do that, work directly from Lemma 3 of \cite{NeuO20}. We will probably be able to get the dimension from the standard trace trick.
    \item In Eq.~\eqref{eq::bad_conditioning} if we want to use Lemma 2 we have to upper bounds tbe probs by $1$ and get $K$ instead of $\sum q_a^{2-\alpha}$. Otherwise we need a bound on $\bE[q_{t,a}^{2-\alpha}\widehat \ell_{t,a}^2]$ directly. 
    \item In the MGR concentration and the proof of Lemma 2 we wrongly do as if the observation probability was uniform across all arms, the only way to make it right seems to use $\min_{X \in \cX}p_t(X)$. \DB{Several errors of this type can be corrected by introducing a forced exploration.}
\end{itemize}

In addition, below are some things that I don't understand and some remarks

\begin{itemize}
    \item In those papers they also use what is our Lemma 2 on the MGR-induced losses directly, not on the "unbiased estimates losses".
    \item I really want to understand what "should" be the right dependency in $d$, if not from a formal lower bound at least from an intuitive point of view.
    \item Kuroki et al. have a worst stochastic bound in $(\log(T))^3+d(\log(T))^2$. I'm wondering why and if we should have the same or if Taira's framework prevents us from that.
\end{itemize}

\paragraph{Ghost samples} Adapting the proofs from previous works we can easily get that 
\[R_T \leq \bE_{X_0 \sim \cD}\left[\wt R_T(X_0)\right] + 2\sum_{t=1}^T \text{bias-MGR}_t, \]
where for any $x\in \cD$ we define 
\[\wt R_T(x) = \sum_{t=1}^T \left\{ \sum_{a=1}^K (q_t(a|x)-\pi_T^\star(a|x)) \langle x, \wt \theta_{t,a} \rangle + cK p_t(x) \right\} \;,\]
where $p_t(x)$ is the observation probability at time $t$ if context $x$ is revealed.

\DB{To me it is on $\wt R_t(x)$ that we have to state and prove Lemma 1.}
