\section{DISCUSSION} \label{sec::discuss}

We proposed an algorithm achieving BoBW regret guarantees in the setting of \emph{linear contextual bandits with paid observations}, with explicit scaling in problem dimensions ($d$, $K$) and parameters ($\lambda_{\text{min}}$, $\deltamin$, $c$).

However, an important limitation, shared with the analysis of Algorithm~2 from \citet{BOBWlinear}, arises in the stochastic setting when the context space is continuous. In such cases, the quantity $\deltamin$ is often zero, which implies that the regret bound remains at $\Theta \spr*{T^{2/3}}$, even though the environment is stochastic and should, in principle, allow for better rates. This issue also affects discrete but finely spaced context spaces, where $\deltamin > 0$ but can be arbitrarily small, leading to overly pessimistic bounds in practice. Nevertheless, \citet{BastaniBK21} demonstrates that under suitable regularity conditions on the context distribution, it is possible to achieve logarithmic regret in continuous settings without any dependence on $\deltamin$. Extending such ideas to our setting, and combining them with BoBW-style guarantees, could lead to improved regret bounds, potentially polylogarithmic or polynomially better than $\sqrt{T}$ or $T^{2/3}$. We believe this is a promising direction for future work.

Finally, as previously discussed, since this setting is novel, there are currently no lower bounds specifically tailored to it. Existing lower bounds only apply to simplified or special cases of our setting. Developing minimax and stochastic lower bounds that are adapted to this setting, precisely capturing all dimensions and parameters, would therefore be an interesting contribution to improve the understanding of this setting.