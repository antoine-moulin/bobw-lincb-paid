\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables
\usepackage[round]{natbib}



\usepackage[hmargin=2cm, vmargin=2cm]{geometry}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2026} with \usepackage[nohyperref]{icml2026} above.
\usepackage{ninecolors}
\usepackage[colorlinks=true,
            linkcolor=blue4,
            filecolor=blue4,
            urlcolor=blue4,
            citecolor=blue4]{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools}
\usepackage{thm-restate}
\usepackage{algorithm}
\usepackage{algorithmic}

% special macros
\input{amacros.tex}
\input{dmacros.tex}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{Lem}{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

\theoremstyle{plain}
\declaretheorem[numberwithin=section,name=Theorem]{theorem}
\declaretheorem[sibling=theorem,name=Proposition]{proposition}
\declaretheorem[sibling=theorem,name=Lemma]{lemma}
\declaretheorem[sibling=theorem,name=Corollary]{corollary}
\theoremstyle{definition}
\declaretheorem[sibling=theorem,name=Definition]{definition}
\declaretheorem[sibling=theorem,name=Assumption]{assumption}
\theoremstyle{remark}
\declaretheorem[sibling=theorem,name=Remark]{remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\begin{document}

\begin{center}
    {\LARGE Linear contextual bandits with paid observations via \texttt{LinExp3}}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Setting}

We consider the problem of linear contextual bandits with paid observations. A learner interacts with an environment for $T$ rounds. Each round $t$ goes as follows. The learner observes the context $X_t \in \cX$ drawn independently from a context distribution $\cD$, and then chooses an action $A_t \in \cA$ to play. After playing, the learner also has the option to pay a cost $c > 0$ to observe the losses of other actions $O_t \subseteq \cA$. The learner then suffers the loss $\ell_t \spr*{X_t, A_t} + c \abs*{O_t}$.

For any time $t \in \sbr*{T}$, context $x \in \cX$, and action $a \in \cA$, the loss $\ell_t \spr*{x, a}$ is generated as $\ell_t \spr*{x, a} = \inp*{x, \theta_{t, a}}$, where $\theta_{t, a} \in \bbR^d$ is the unknown parameter. We assume that $\norm*{\theta_{t, a}}_2 \leq 1$ for all $t \in \sbr*{T}$ and $a \in \cA$, and $\norm*{x}_2 \leq 1$ for all $x \in \cX$. The goal of the learner is to minimize the cumulative regret defined as
%
\begin{equation*}
    R_T = \bbE \sbr*{\sumtT \ell_t \spr*{X_t, A_t} - \sumtT \ell_t \spr*{X_t, \pi_T^\star \spr*{X_t}}} + c\,\bbE \sbr*{\sumtT \abs*{O_t}},
\end{equation*}
%
where $\pi_T^\star\colon x \in \cX \mapsto \argmin_{a \in \cA} \bbE \sbr*{\sumtT \ell_t \spr*{x, a}}$ is the best policy in hindsight. Note that we can rewrite $c\,\bbE \sbr*{\sumtT \abs*{O_t}} = c K\, \bbE \sbr*{\sumtT p_t}$. In the following, we denote $\bbE_t$ the conditional expectation given the history up to time $t-1$. We also denote $\Sigma = \bbE \sbr*{X X\transpose}$ the covariance matrix of the context distribution.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm}

We adapt the \texttt{RobustLinExp3} algorithm proposed by \citet{NeuO20} to our setting. The only differences are that we remove the uniform exploration and use the probability of observation instead, and the importance weights used in the estimators are based on the probability of observation instead of the probability of action selection. The details are given in Algorithm~\ref{alg:robustlinexp3}.

\begin{algorithm}[ht]
    \caption{\texttt{RobustLinExp3} for linear contextual bandits with paid observations}
    \label{alg:robustlinexp3}
    \begin{algorithmic}
        \STATE \textbf{Input:} Action set $\cA$, context set $\cX$, time horizon $T$, learning rate $\eta > 0$.
        \STATE \textbf{Initialize:} Set $\theta_{0, a} = 0$ for all $a \in \cA$.
        \FOR{$t=1$ to $T$}
            \STATE Observe context $X_t \in \cX$.
            \STATE For any action $a \in \sbr*{K}$, compute probability distribution over actions $q_t \spr*{\cdot \given X_t} \propto \exp \spr*{- \eta \sum_{s=0}^{t-1} \inp*{X_t, \hat{\theta}_{s, a}}}$.
            \STATE Draw action $A_t$ from distribution $q_t \spr*{\cdot \given X_t}$.
            \STATE Compute probability $p_t$.
            \STATE For any action $a \in \cA$, observe $\ell_t \spr*{X_t, a}$ with probability $p_t$, and denote $O_t$ the set of observed actions.
            \STATE Suffer the loss $\ell_t \spr*{X_t, A_t} + c \abs{O_t}$.
            \STATE For any action $a \in \sbr*{K}$, compute the estimated parameter $\hat{\theta}_{t, a} = \frac{\mathds{1} \scbr*{a \in O_t}}{p_t} \Sigma^{-1} X_t \ell_t \spr*{X_t, a}$.
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis}

We start by noting that the estimator $\hat{\theta}_{t, a}$ used in Algorithm~\ref{alg:robustlinexp3} is an unbiased estimator of $\theta_{t, a}$. Indeed, by definition and a tower rule, we have
%
\begin{equation*}
    \bbE_t \sbr*{\hat{\theta}_{t, a}} = \bbE_t \sbr*{\bbE_t \sbr*{\frac{\mathds{1} \scbr*{a \in O_t}}{p_t} \given X_t} \Sigma^{-1} X_t \ell_t \spr*{X_t, a}} = \Sigma^{-1} \bbE_t \sbr*{X_t \ell_t \spr*{X_t, a}} = \Sigma^{-1} \bbE_t \sbr*{X_t X_t\transpose} \theta_{t, a} = \theta_{t, a},
\end{equation*}
%
where we used the fact that contexts are drawn independently, so $\bbE_t \sbr*{X_t X_t\transpose} = \Sigma$. Using Lemma~3 from \citet{NeuO20}, we can then introduce a ghost sample $X_0 \sim \cD$, and write the regret as
%
\begin{align*}
    R_T = \bbE \sbr*{\sumtT \sumaK \spr*{q_t \spr*{a \given X_0} - \pi_T^\star \spr*{a \given X_0}} \widehat{\ell}_t \spr*{X_0, a}} + c K\, \bbE \sbr*{\sumtT p_t},
\end{align*}
%
where we defined $\widehat{\ell}_t \spr*{X_0, a} = \inp*{X_0, \hat{\theta}_{t, a}}$. This shows it is enough to get an almost-sure bound on the same quantity without the expectation over $X_0$. In the following, we consider a fixed context $x \in \cX$.

Denote $w_t \spr*{x, a} = \exp \spr{- \eta \sum_{s=1}^{t-1} \inp{x, \hat{\theta}_{s, a}}}$, and $W_t \spr*{x} = \sum_{a \in \cA} w_t \spr*{x, a}$. We can then write $q_t \spr*{a \given x} = w_t \spr*{x, a} / W_t \spr*{x}$. Using the same analysis as in the proof of Theorem~1 in \citet{NeuO20}, we have
%
\begin{align*}
    \log \spr*{\frac{W_{T+1} \spr*{x}}{W_1 \spr*{x}}} &\geq \log \spr*{\frac{w_{T+1} \spr*{x, \pi_T^\star \spr*{x}}}{W_1 \spr*{x}}}\\
    &= - \eta \sumtT \inp*{x, \widehat{\theta}_{t, \pi_T^\star \spr*{x}}} - \log K,
\end{align*}
%
where we used $W_1 \spr*{x} = K$. On the other hand, we also have
%
\begin{align*}
    \log \spr*{\frac{W_{t+1} \spr*{x}}{W_t \spr*{x}}} &= \log \spr*{\sum_{a \in \cA} \frac{w_{t+1} \spr*{x, a}}{W_t \spr*{x}}}\\
    &= \log \spr*{\sum_{a \in \cA} q_t \spr*{a \given x} \exp \spr*{- \eta \inp*{x, \hat{\theta}_{t, a}}}}.
\end{align*}
%
Assuming we have $\eta \inp{x, \hat{\theta}_{t, a}} \geq - 1$ for all $t \in \sbr*{T}$ and $a \in \cA$, we can use the inequality $e^{-z} \leq 1 - z + z^2$ for all $z \leq 1$ to get
%
\begin{align*}
    \log \spr*{\frac{W_{t+1} \spr*{x}}{W_t \spr*{x}}} &\leq \log \spr*{\sum_{a \in \cA} q_t \spr*{a \given x} \spr*{1 - \eta \inp*{x, \hat{\theta}_{t, a}} + \eta^2 \inp*{x, \hat{\theta}_{t, a}}^2}}\\
    &= \log \spr*{1 - \eta \sum_{a \in \cA} q_t \spr*{a \given x} \inp*{x, \hat{\theta}_{t, a}} + \eta^2 \sum_{a \in \cA} q_t \spr*{a \given x}  \inp*{x, \hat{\theta}_{t, a}}^2},
\end{align*}
%
where we also used $\sum_{a \in \cA} q_t \spr*{a \given x} = 1$. Then, note that by Jensen's inequality, $\sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}}^2 \geq \spr*{\sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}}}^2$. Therefore,
%
\begin{align*}
    &- \eta \sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}} + \eta^2 \sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}}^2\\
    &\qquad\geq - \eta \sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}} + \eta^2 \spr*{\sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}}}^2 \\
    &\qquad= \spr*{\eta \sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}} - \frac12}^2 - \frac14 \\
    &\qquad\geq -\frac14 > -1.
\end{align*}
%
Hence the log argument in the previous inequality is always larger than $-1$ (for any $\eta > 0$), so we can apply the inequality $\log \spr*{1 + u} \leq u$ valid for any $u$, and get
%
\begin{equation*}
    \log \spr*{\frac{W_{t+1} \spr*{x}}{W_t \spr*{x}}} \leq - \eta \sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}} + \eta^2 \sum_{a \in \cA} q_t \spr*{a \given x}  \inp{x, \hat{\theta}_{t, a}}^2.
\end{equation*}
%
Noticing that $\sumtT \log \spr*{\frac{W_{t+1} \spr*{x}}{W_t \spr*{x}}} = \log \spr*{\frac{W_{T+1} \spr*{x}}{W_1 \spr*{x}}}$, we can sum the previous upper bound over $t$, combine it with the previous lower bound, and divide by $\eta$ to get
%
\begin{equation*}
    \sumtT \sum_{a \in \cA} q_t \spr*{a \given x} \inp{x, \hat{\theta}_{t, a}} - \sumtT \inp{x, \hat{\theta}_{t, \pi_T^\star \spr*{x}}} \leq \frac{\log K}{\eta} + \eta \sumtT \sum_{a \in \cA} q_t \spr*{a \given x}  \inp{x, \hat{\theta}_{t, a}}^2.
\end{equation*}
%
Therefore, the regret can be upper bounded as
%
\begin{equation*}
    R_T \leq \frac{\log K}{\eta} + \eta \bbE \sbr*{\sumtT \sum_{a \in \cA} q_t \spr*{a \given X_0} \inp{X_0, \hat{\theta}_{t, a}}^2} + c K\, \bbE \sbr*{\sumtT p_t}.
\end{equation*}
%
Looking at the variance for a given $t$, we have
%
\begin{align*}
    \bbE_t \sbr*{\sum_{a \in \cA} q_t \spr*{a \given X_0}  \inp{X_0, \hat{\theta}_{t, a}}^2} &= \bbE_t \sbr*{\sum_{a \in \cA} q_t \spr*{a \given X_0} \frac{\mathds{1} \scbr*{a \in O_t}}{p_t^2} \ell_t \spr*{X_t, a}^2 \inp*{X_0, \Sigma^{-1} X_t}^2}\\
    &= \bbE_t \sbr*{\sum_{a \in \cA} q_t \spr*{a \given X_0} \frac{1}{p_t} \ell_t \spr*{X_t, a}^2 \inp*{X_0, \Sigma^{-1} X_t}^2}\\
    &\leq \bbE_t \sbr*{\sum_{a \in \cA} q_t \spr*{a \given X_0} \frac{1}{p_t} \inp*{X_0, \Sigma^{-1} X_t}^2}
\end{align*}
%
where we used $\bbE_t \sbr*{\mathds{1} \scbr*{a \in O_t} \given X_0, X_t} = p_t$ in the second line, and $\ell_t \spr*{X_t, a}^2 \leq 1$ in the third line. Then, writing $\inp{X_0, \Sigma^{-1} X_t}^2 = X_0\transpose \Sigma^{-1} X_t X_t\transpose \Sigma^{-1} X_0 = \tr \spr*{\Sigma^{-1} X_t X_t\transpose \Sigma^{-1} X_0 X_0\transpose}$, we can use $\bbE_t \sbr*{X_t X_t\transpose \given X_0} = \Sigma$ by independence of $X_0$ and $X_t$ to obtain
%
\begin{equation*}
    \bbE_t \sbr*{\sum_{a \in \cA} q_t \spr*{a \given X_0} \inp{X_0, \hat{\theta}_{t, a}}^2} \leq \frac{1}{p_t} \bbE_t \sbr*{\tr \spr*{\Sigma^{-1} X_0 X_0\transpose}} = \frac{d}{p_t}.
\end{equation*}
%
Plugging this back into the regret bound, we get
%
\begin{equation*}
    R_T \leq \frac{\log K}{\eta} + \eta d \bbE \sbr*{\sumtT \frac{1}{p_t}} + c K\, \bbE \sbr*{\sumtT p_t}.
\end{equation*}
%
Taking a constant $p_t = p$, we can optimize the previous bound by choosing $p$ such that $\eta d / p = c K p$, i.e., $p = \sqrt{\eta d / (c K)}$. This gives
%
\begin{equation*}
    R_T \leq \frac{\log K}{\eta} + 2 \sqrt{\eta d c K} T.
\end{equation*}
%
Finally, choosing $\eta = \spr*{\frac{\log K}{\sqrt{d c K} T}}^{2/3}$ gives \antoine{up to a missing minimum eigenvalue to enforce $\eta \inp{x, \hat{\theta}_{t, a}} \geq -1$}
%
\begin{equation*}
    R_T = \cO \spr*{T^{2/3}}.
\end{equation*}


\bibliography{biblio}
\bibliographystyle{plainnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix


\end{document}
